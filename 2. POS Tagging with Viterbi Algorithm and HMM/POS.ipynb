{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1pw_A7PlnRfdXw2hKbdeKcsgFMl_3yirt","authorship_tag":"ABX9TyPZhJ8gtRwJNS0YR2a+cBMf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Loading Libraries and the Dataset"],"metadata":{"id":"bIidq1BKzieU"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9k-OZzKTuTt6","executionInfo":{"status":"ok","timestamp":1691338239410,"user_tz":-210,"elapsed":16745,"user":{"displayName":"Kimia Marvi","userId":"07453454028628586545"}},"outputId":"83e5ca49-1631-4a05-f40c-a04e2408760f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import numpy as np\n","from collections import Counter\n","import nltk\n","from nltk.util import ngrams"],"metadata":{"id":"5LEWvDYd6nxd"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-kASfk2Wn_P2"},"outputs":[],"source":["train = np.loadtxt('/data/Train.txt', dtype='str', comments=None)\n","val = np.loadtxt('/data/Val.txt', dtype='str', comments=None)\n","test = np.loadtxt('/data/Test.txt', dtype='str', comments=None)"]},{"cell_type":"markdown","source":["# The Dataset Properties"],"metadata":{"id":"vCKEqAY5z9E7"}},{"cell_type":"code","source":["train_vocab = set(train[:,0])\n","val_vocab = set(val[:,0])\n","test_vocab = set(test[:])\n","labels = set(train[:,1])"],"metadata":{"id":"qOxwmI-j8YF5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Number of labels: ', len(labels))\n","print('Number of words in train set: ', len(train_vocab))\n","print('Number of words in validation set: ', len(val_vocab))\n","print('Number of words in test set: ', len(test_vocab))\n","print('Number of words in validation set which are not in train set: ', len(val_vocab - train_vocab))\n","print('Number of words in test set which are not in train set: ', len(test_vocab - train_vocab))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JN710vTjBaU-","executionInfo":{"status":"ok","timestamp":1691340469322,"user_tz":-210,"elapsed":335,"user":{"displayName":"Kimia Marvi","userId":"07453454028628586545"}},"outputId":"c68f2e2f-9866-4f50-803b-6d4e5cb05485"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of labels:  23\n","Number of words in train set:  18949\n","Number of words in validation set:  16235\n","Number of words in test set:  10594\n","Number of words in validation set which are not in train set:  8927\n","Number of words in test set which are not in train set:  4971\n"]}]},{"cell_type":"markdown","source":["# Defining Vocabulary"],"metadata":{"id":"Lf8Z3qQB0Rre"}},{"cell_type":"code","source":["# Create a vocabulary set consisting of the k most frequent words in the text\n","vocab_size  = 8000\n","all_words = list(train[:,0])\n","word_freq = Counter(all_words)\n","vocab = set([word for word, freq in word_freq.most_common(vocab_size)])\n","\n","# Replace all words not in the vocabulary with the <unk> token\n","train[:,0] = np.array([word if word in vocab else '<unk>' for word in train[:,0]])\n","val[:,0] = np.array([word if word in vocab else '<unk>' for word in val[:,0]])\n","test = np.array([word if word in vocab else '<unk>' for word in test])"],"metadata":{"id":"P_u4JVgnPTJ4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Separating Sentences"],"metadata":{"id":"p_X_Pzhx0czE"}},{"cell_type":"code","source":["def padding(dataset, delm=[':', ';', '?', '.', '!', 'ØŸ']):\n","    padded = [begin_padding]\n","    for x in dataset:\n","        padded.append(x)\n","        if (isinstance(x, np.ndarray) and x[0] in delm) or (isinstance(x, str) and (x in delm)):\n","            padded.append(end_padding)\n","            padded.append(begin_padding)\n","    return np.array(padded[:-1])"],"metadata":{"id":"lBWr1GJkLsVc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["begin_padding = np.array(['<s>','start'], dtype='str')\n","end_padding = np.array(['</s>','end'], dtype='str')\n","train = padding(train)\n","val = padding(val)\n","begin_padding = '<s>'\n","end_padding = '</s>'\n","test = padding(test)"],"metadata":{"id":"oVmgM0rSL88l"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Emition"],"metadata":{"id":"p__qvnsuWgHW"}},{"cell_type":"code","source":["label_count = Counter(list(train[:,1]))\n","emition_count = Counter(tuple(x) for x in train)"],"metadata":{"id":"ckTiaRlzDtjL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def emition_probs(emition_count, label_count, k = 1):\n","    V = vocab_size + 3\n","    emitions_freq = Counter()\n","\n","    # Apply additive smoothing with parameter k to the n-gram frequencies\n","    for observation, freq in emition_count.items():\n","        count = freq + k\n","        prefix_count = label_count[observation[1]]\n","        total_count = prefix_count + k*V\n","        emitions_freq[observation] = np.log(count / total_count)\n","\n","    return emitions_freq"],"metadata":{"id":"nLsd_fq5ULwV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Transition"],"metadata":{"id":"xcn-jpkeBod5"}},{"cell_type":"code","source":["def generate_ngrams(labels, n=2):\n","    # Generate n-grams from the padded text using the nltk ngrams function\n","    ngrams_list = list(ngrams(labels, n))\n","\n","    # Count the frequency of each n-gram using the Counter function from the collections module\n","    ngrams_count = Counter(ngrams_list)\n","    return ngrams_count\n","\n","transition_count = generate_ngrams(train[:,1])\n","\n","def ngram_probs(bigrams, unigrams, k = 1):\n","    V = len(unigrams)\n","    ngrams_freq = Counter()\n","\n","    # Apply additive smoothing with parameter k to the n-gram frequencies\n","    for ngram, freq in bigrams.items():\n","        count = freq + k\n","        prefix_count = unigrams[ngram[0]]\n","        total_count = prefix_count + k*V\n","        ngrams_freq[ngram] = np.log(count / total_count)\n","\n","    return ngrams_freq"],"metadata":{"id":"z5UwtADc_2W7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Viterbi Algorithm"],"metadata":{"id":"bletPlAyaofh"}},{"cell_type":"code","source":["def viterbi(obs, states, start_prob, trans_prob, emit_prob, zero_prob_emit, zero_prob_trans):\n","    # Initialize matrices\n","    T = len(obs)\n","    N = len(states)\n","    viterbi_mat = np.zeros((N, 2))\n","    backpointer = np.zeros((N, T), dtype=int)\n","\n","    # Precompute transition and emission probabilities\n","    trans_prob_mat = np.array([[trans_prob.get((states[s0], states[s]), zero_prob_trans[states[s0]]) for s in range(N)] for s0 in range(N)])\n","    emit_prob_mat = np.array([[emit_prob.get((obs[t], states[s]), zero_prob_emit[states[s]]) for s in range(N)] for t in range(T)])\n","    start_prob_mat = np.array([start_prob[states[s0]] for s0 in range(N)])\n","\n","    # Initialization step\n","    viterbi_mat[:, 0] = start_prob_mat + emit_prob_mat[0]\n","\n","    # Forward step\n","    for t in range(1, T):\n","        temp = viterbi_mat[:, 0] + trans_prob_mat.T + emit_prob_mat[t].reshape(N,1)\n","        viterbi_mat[:, 1] = np.max(temp, axis=1)\n","        backpointer[:, t] = np.argmax(temp, axis=1)\n","        viterbi_mat[:, 0] = viterbi_mat[:, 1]\n","\n","    # Backward step\n","    best_path_prob = np.max(viterbi_mat[:, 1])\n","    best_path_pointer = np.argmax(viterbi_mat[:, 1])\n","    best_path = [best_path_pointer]\n","\n","    for t in range(T - 2, -1, -1):\n","        best_path_pointer = backpointer[best_path_pointer, t + 1]\n","        best_path.insert(0, best_path_pointer)\n","\n","    return best_path, best_path_prob\n"],"metadata":{"id":"srZ32DgXYDcj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Putting it all together"],"metadata":{"id":"D8pfz1eo02oG"}},{"cell_type":"code","source":["def run(add_k, final=False):\n","    emition_prob = emition_probs(emition_count, label_count, add_k)\n","    transition_prob = ngram_probs(transition_count, label_count, add_k)\n","    start_prob = {}\n","    for item, count in label_count.items():\n","        count = (item == 'start') * label_count['start']\n","        start_prob[item] = np.log((count + add_k) / (label_count['start'] + add_k * len(list(label_count.keys()))))\n","\n","    zero_prob_emit = {}\n","    for label, count in label_count.items():\n","        zero_prob_emit[label] = np.log(add_k / (label_count[label] + add_k*(vocab_size+3)))\n","    zero_prob_emit['start'] = float('-inf')\n","    zero_prob_emit['end'] = float('-inf')\n","\n","    zero_prob_trans = {}\n","    for label, count in label_count.items():\n","        zero_prob_trans[label] = np.log(add_k / (label_count[label] + add_k*len(label_count)))\n","    zero_prob_trans['end'] = float('-inf')\n","\n","    states = list(label_count.keys())\n","\n","    if final:\n","        best_path_test, _ = viterbi(list(test), states, start_prob, transition_prob, emition_prob, zero_prob_emit, zero_prob_trans)\n","        for i in range(len(test)):\n","            best_path_test[i] = states[best_path_test[i]]\n","        return best_path_test\n","\n","    best_path_train, _ = viterbi(list(train[:,0]), states, start_prob, transition_prob, emition_prob, zero_prob_emit, zero_prob_trans)\n","    best_path_val, _ = viterbi(list(val[:,0]), states, start_prob, transition_prob, emition_prob, zero_prob_emit, zero_prob_trans)\n","\n","    true = 0\n","    tot=0\n","    for i in range(len(best_path_train)):\n","        if train[i][1] != 'start' and train[i][1] != 'end':\n","            tot += 1\n","            true += train[i][1] == states[best_path_train[i]]\n","    train_acc = true/tot\n","\n","    true = 0\n","    tot=0\n","    for i in range(len(best_path_val)):\n","        if val[i][1] != 'start' and val[i][1] != 'end':\n","            tot += 1\n","            true += val[i][1] == states[best_path_val[i]]\n","    val_acc = true/tot\n","\n","    return train_acc, val_acc"],"metadata":{"id":"xugO1_TkAESO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Evaluation"],"metadata":{"id":"mt1Z7sEt1S1M"}},{"cell_type":"code","source":["for i in [0, -1, -2, -3, -4, -5, -8]:\n","    print('train and validation accuracy for k = ', 10**i, ' : ', run(10**i))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BfPDkYlzmKL9","executionInfo":{"status":"ok","timestamp":1691341293738,"user_tz":-210,"elapsed":361190,"user":{"displayName":"Kimia Marvi","userId":"07453454028628586545"}},"outputId":"237c9698-3c20-4ba3-cad6-7d3943f6d69c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["train and validation accuracy for k =  1  :  (0.9477778547618497, 0.9254818787226093)\n","train and validation accuracy for k =  0.1  :  (0.9568465784429202, 0.9123607112244225)\n","train and validation accuracy for k =  0.01  :  (0.9582284425352394, 0.9145750456053979)\n","train and validation accuracy for k =  0.001  :  (0.9585825692664188, 0.9179542407525441)\n","train and validation accuracy for k =  0.0001  :  (0.9586172120988168, 0.9189707466098156)\n","train and validation accuracy for k =  1e-05  :  (0.9586210613024165, 0.9192454779225917)\n","train and validation accuracy for k =  1e-08  :  (0.9586210613024165, 0.9192949295588914)\n"]}]},{"cell_type":"code","source":["test_result = run(1, True)"],"metadata":{"id":"FlTPealvavzw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_pos = []\n","for i in range(len(test)):\n","    if test[i] != '<s>' and test[i] != '</s>':\n","        test_pos.append(test_result[i])"],"metadata":{"id":"8NitqNfPa7Rl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["path = 'y_test.txt'\n","with open(path, 'w') as file:\n","    for item in test_pos:\n","        file.write(str(item) + '\\n')"],"metadata":{"id":"8gkx0A9WaX0p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"8G5Z7f5Qc1k1"},"execution_count":null,"outputs":[]}]}